# A simple example that copies from a topic to a SQLite database.
# The first few settings are required for all connectors:
# a name, the connector class to run, and the maximum number of tasks to create:
name=header-sink
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
tasks.max=1



# The topics to consume from - required for sink connectors like this one
topics=headers

# Configuration specific to the JDBC sink connector.
# We want to connect to a SQLite database stored in the file test.db and auto-create tables.
connection.url=jdbc:mysql://mysql:3306/ksql_poc?nullCatalogMeansCurrent=true
connection.user=root
connection.password=confluent
dialect.name=MySqlDatabaseDialect
auto.create=false

key.converter=org.apache.kafka.connect.storage.StringConverter
#value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://sr:8081
#key.converter.schemas.enable=falsed
#value.converter.schemas.enable=false

transforms:ValueToKey
transforms.ValueToKey.type:org.apache.kafka.connect.transforms.ValueToKey
transforms.ValueToKey.fields:GUID


insert.mode=update
table.name.format=materialized_view

pk.mode=record_key
#pk.mode=record_value
pk.fields=GUID

# Define when identifiers should be quoted in DDL and DML statements.
# The default is 'always' to maintain backward compatibility with prior versions.
# Set this to 'never' to avoid quoting fully-qualified or simple table and column names.
#quote.sql.identifiers=always